{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Credit Card Behaviour Score Prediction\n",
        "### Forward-Looking Risk Classification using Financial Behavior Features\n",
        "\n",
        [cite_start]"**Author:** Aditya Kumar Verma [cite: 2] \n",
        [cite_start]"**Objective:** Predict customer default in the next billing cycle using behavioral data[cite: 5]. [cite_start]This model prioritizes **Recall** and the **$F_2$ Score** (targeting 0.6092) to support proactive risk management[cite: 6, 74]."
      ],
      "metadata": {
        "id": "SuGpT_fxlNGx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_libs"
      },
      "outputs": [],
      "source": [
        "# Install required libraries if not already present\n",
        "!pip install lightgbm xgboost imbalanced-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import Required Libraries"
      ],
      "metadata": {
        "id": "Uz73gkC1lSP4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9az3EyObEZ9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    recall_score, precision_score, fbeta_score, \n",
        "    confusion_matrix, classification_report, roc_auc_score\n",
        ")\n",
        "from lightgbm import LGBMClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Datasets and Basic Imputation"
      ],
      "metadata": {
        "id": "hiFXeMFQlUWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmUXI7AudTG5"
      },
      "outputs": [],
      "source": [
        "# Load using relative paths for GitHub portability\n",
        "train_df = pd.read_csv(\"train_dataset_final1.csv\")\n",
        "val_df = pd.read_csv(\"validate_dataset_final.csv\")\n",
        "\n",
        [cite_start]"# Handle missing values identified in EDA [cite: 18]\n",
        "train_df['age'].fillna(train_df['age'].median(), inplace=True)\n",
        "\n",
        [cite_start]"print(f\"Train set: {train_df.shape[0]} customers\") [cite: 13]\n",
        [cite_start]"print(f\"Validation set: {val_df.shape[0]} customers\") [cite: 13]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feature Engineering\n",
        [cite_start]"Creating behavioral metrics to capture risk patterns over time[cite: 41, 42]."
      ],
      "metadata": {
        "id": "feature_eng_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_eng_code"
      },
      "outputs": [],
      "source": [
        [cite_start]"# 1. Utilization Ratio: Average bill relative to credit limit [cite: 43]\n",
        "train_df['Utilization_Ratio'] = train_df['AVG_Bill_amt'] / train_df['LIMIT_BAL']\n",
        "\n",
        [cite_start]"# 2. Delinquency Streak: Count of months with payment delays (PAY >= 1) [cite: 43]\n",
        "pay_cols = ['pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']\n",
        "train_df['Delinquency_Streak'] = (train_df[pay_cols] >= 1).sum(axis=1)\n",
        "\n",
        [cite_start]"# 3. Repayment Volatility (Standard Deviation) [cite: 44]\n",
        "pay_amt_cols = ['pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']\n",
        "train_df['Repayment_Std'] = train_df[pay_amt_cols].std(axis=1)\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Visualizing Feature Correlations"
      ],
      "metadata": {
        "id": "viz_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viz_code"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "correlation = train_df[['next_month_default', 'LIMIT_BAL', 'age', 'Utilization_Ratio', 'Delinquency_Streak', 'Repayment_Std']].corr()\n",
        "sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Heatmap: Target vs. Engineered Features')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Training (LightGBM)\n",
        [cite_start]"Applying class weights to handle imbalance and training the best-performing model[cite: 49, 51]."
      ],
      "metadata": {
        "id": "model_train_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_train_code"
      },
      "outputs": [],
      "source": [
        "X = train_df.drop(['Customer_ID', 'next_month_default'], axis=1)\n",
        "y = train_df['next_month_default']\n",
        "\n",
        "# Split for internal validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        [cite_start]"# Initialize LightGBM with balanced class weights [cite: 49, 51]\n",
        "lgbm = LGBMClassifier(class_weight='balanced', random_state=42)\n",
        "lgbm.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Threshold Optimization & Final Evaluation\n",
        [cite_start]"Applying the optimized threshold of **0.36** to maximize the **$F_2$ score**[cite: 54, 78]."
      ],
      "metadata": {
        "id": "eval_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval_code"
      },
      "outputs": [],
      "source": [
        "# Get predicted probabilities\n",
        "y_probs = lgbm.predict_proba(X_test)[:, 1]\n",
        "\n",
        [cite_start]"# Apply optimized threshold from report [cite: 54, 78]\n",
        "threshold = 0.36\n",
        "y_pred_optimized = (y_probs >= threshold).astype(int)\n",
        "\n",
        "print(\"--- Optimized Model Performance ---\")\n",
        [cite_start]"print(f\"Recall: {recall_score(y_test, y_pred_optimized):.2f}\") [cite: 75]\n",
        [cite_start]"print(f\"Precision: {precision_score(y_test, y_pred_optimized):.2f}\") [cite: 76]\n",
        [cite_start]"print(f\"F2 Score: {fbeta_score(y_test, y_pred_optimized, beta=2):.4f}\") [cite: 74]\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_optimized))"
      ]
    }
  ]
}
